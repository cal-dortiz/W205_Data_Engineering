{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> W205 Data Engineering </h1></center>\n",
    "<center><h3> Assignment 2          </h3></center>\n",
    "<center><h3> Dan Ortiz             </h3></center>\n",
    "<center><h3> Thursday 4:00PM       </h3></center>\n",
    "<center><h3> 10/25/2020            </h3><Center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Load and Transform Data From Kafka to Spark </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load dependent packages into notebook\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import explode, split\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, BooleanType\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Change settings in spark to allow transfer to pandas dataframe\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Smash the subscribe button for the assessment topic and read all the messages in from it.\n",
    "\n",
    "raw_assessments = spark \\\n",
    "   .read \\\n",
    "   .format(\"kafka\") \\\n",
    "   .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "   .option(\"subscribe\", \"assessments\") \\\n",
    "   .option(\"startingOffsets\", \"earliest\") \\\n",
    "   .option(\"endingOffsets\", \"latest\") \\\n",
    "   .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caches the data, forcing the previous block to fully execute, verifies syntax is good.\n",
    "\n",
    "raw_assessments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3280"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the expected number of messages/events came through. Expect 3280 messages.\n",
    "\n",
    "raw_assessments.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Cast all values into string for parsing\n",
    "\n",
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/session.py:351: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "#Converts data into an RDD then into dataframe.\n",
    "#This helps convert the nested JSON files into lists and lists of lists. This \n",
    "#will help as we define the schema in the next section.\n",
    "\n",
    "extracted_assessments = assessments.rdd.map(lambda x: json.loads(x.value)).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Writes the raw dataframe to a parquet file\n",
    "\n",
    "raw_assessments.write.parquet(\"/tmp/assessments_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Build the Final Schema </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decisions with the Final Schema\n",
    "\n",
    "I scoped the schema with the mindset of answering business decisions. Information source includes higher level information that is unique to the submission and lower level information on the assessment itself. The thinking is there may be a difference between students taking the class for certification and those who are not. The goal of this schema is to clean filter out what is not needed to increase efficiency and performance. The raw data is saved, and another schema can be designed if and when it is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Defines the final schema\n",
    "\n",
    "final_schema = StructType([StructField('user_exam_id', StringType(), True),\n",
    "                           StructField('base_exam_id', StringType(), True),\n",
    "                           StructField('exam_name', StringType(), True),\n",
    "                           StructField('certification', StringType(), True),\n",
    "                           StructField('max_attempts', StringType(), True),\n",
    "                           StructField('started_at', StringType(), True),\n",
    "                           StructField('sequences', StructType([\n",
    "                               StructField('attempt', IntegerType(), True),\n",
    "                               StructField('counts', StructType([\n",
    "                                   StructField('submitted', IntegerType(), True),\n",
    "                                   StructField('correct', IntegerType(), True),\n",
    "                                   StructField('total', IntegerType(), True),\n",
    "                                   StructField('unanswered', IntegerType(), True),\n",
    "                                   StructField('all_correct', BooleanType(), True)\n",
    "                                   ]))\n",
    "                               ]))\n",
    "                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Transforms the casted data into a dataframe using the schema\n",
    "#defined above\n",
    "\n",
    "focused_extracted_assessments = assessments.rdd.map(lambda x: json.loads(x.value)).toDF(schema=final_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Regesters the previously generated data frame as a temp table\n",
    "#so we can query the data with PySpark.\n",
    "\n",
    "focused_extracted_assessments.registerTempTable('focused_assessments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Buisness Analysis </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What the parts of the pipeline do\n",
    "\n",
    "#### Docker and Docker Compose\n",
    "\n",
    "Docker builds containers which provide a space for an application to run isolated from the rest of the system. This compartmentalization allows the applications we are using in the pipeline to be modular and compartmentalized. Docker-compose makes it easy to define the containers and bridges within a .yml file, allowing the user to spin up or shut down the cluster easily. In addition, this portability greatly enhances repeatability and reproducibility of results by allowing the exact environment an experiment was ran to be shared.\n",
    "\n",
    "#### cURL\n",
    "\n",
    "The cURL CLI command allows us to send and receive data from the command line. This allows the pipeline to get the JSON file and is the start of the pipeline.\n",
    "\n",
    "#### Kafka and JQ\n",
    "\n",
    "JQ is used to strip the initial document structure allowing Kafka to send messages\\events at the individual assessment level instead of the document level.\n",
    "\n",
    "Kafka is used to set up a topic and send messages in that topic. The topic for this document is \"assessments\". Kafka was fed from the JQ function, it will send its messages at the assessment level. In addition it sets the number of partitions and replications of the data.\n",
    "\n",
    "#### Zookeeper\n",
    "\n",
    "Zookeeper synchronizes services for distributed application. It keeps track the status of the Kafka cluster, its topics, partitions etc. It acts as the broker between Kafka and Spark.\n",
    "\n",
    "#### Spark (PySpark)\n",
    "\n",
    "Spark, and the PySpark implementation used, subscribes to the topic generated by Kafka and reads it. Once the data is read it will set a default schema to the data. In addition, it is being used to design a more tailored schema for the analysis and build an interactable dataframe.\n",
    "\n",
    "#### Spark (SparkSql)\n",
    "\n",
    "SparkSql is the querying language used to query data from the dataframe generated by Spark.\n",
    "\n",
    "#### Pandas\n",
    "\n",
    "Pandas is used to take convert the text output of SparkSql to a prettier table format for consumption.\n",
    "\n",
    "#### What I set up myself \n",
    "\n",
    "Many components of the pipeline were directly provided or were reviewed in class. The following items are the items I set up or modified myself\n",
    "\n",
    "   - Modified the week 8 docker-compose file, opening port 8888 allowing Jupyter notebooks to connect.\n",
    "   - Implemented a shell command to automate the CLI portion of the pipeline\n",
    "   - Modified and implemented the framework of the pipeline reviewed in lecture to work with this data\n",
    "   - Named the topic \"assessments\"\n",
    "   - Developed the final schema, allowing me to query the dataframe to answer the provided questions\n",
    "   - Install docker-compose from the CLI\n",
    "   - Enabled Arrow, allowing the creation of pandas DF from the spark.sql query\n",
    "\n",
    "\n",
    "#### Issues and Findings In the Data \n",
    "\n",
    "   - We have unique exam id's but not unique student id's. This limits our understanding of the relationship between student and exam_id. This prevents us from understanding on how students interact with courses, or if students repeat courses\n",
    "   - There are a lot of duplicate assessments in the data. We need to be careful on how we query the data to ensure we are not filtering this out.\n",
    "   - It is difficult to understand what the data means in the \"questions\" portions of the data without documentation. For example, in the first assessment, question 0 shows status of incomplete, however the submissions show the user submitted answers to the questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many assesstments are in the dataset? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assumptions and Thinking \n",
    "\n",
    "##### Assumptions\n",
    "\n",
    "   - The intent is to understand the number of tests, not the number of messages. There could be duplicate tests in the pipeline.\n",
    "   - All students enrolled in the class took an assessment\n",
    "   - Students only take the class once\n",
    "   - There is only one assessment in the class\n",
    "   \n",
    "##### Thinking\n",
    "\n",
    "   - The second query validates each exam_user_id is unique to each exam, and not to the user themselves because it is equal to the number of unique exam ids in the first query.\n",
    "   - This indicates we know of individual exams, but we do not know if an individual took more than one class.\n",
    "\n",
    "##### Findings\n",
    "\n",
    "There are 3,280 total records in the json File. The file index starts at 0 and ends at 3,279. However, there are only 3,242 unique user_exam_id, indicating there are multiple duplicate records. Further analysis should be sure to use \"DISTINCT\" function when counting records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_exams</th>\n",
       "      <th>unique_exams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3280</td>\n",
       "      <td>3242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_exams  unique_exams\n",
       "0         3280          3242"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assessment_qty = spark.sql(\"SELECT COUNT(user_exam_id) AS total_exams, \\\n",
    "                            COUNT(DISTINCT user_exam_id) AS unique_exams \\\n",
    "                            FROM focused_assessments\")\n",
    "\n",
    "assessment_qty_pdf = assessment_qty.select(\"*\").toPandas()\n",
    "assessment_qty_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0   3242"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo = spark.sql(\"SELECT COUNT(*) AS count\\\n",
    "                       FROM (SELECT DISTINCT user_exam_id, exam_name \\\n",
    "                       FROM focused_assessments)\")\n",
    "\n",
    "combo_pdf = combo.select(\"*\").toPandas()\n",
    "combo_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the name of your Kafka topic? How did you come up with that name? \n",
    "\n",
    "I named my Kafka topic \"assessments\" for a few reasons. First, each message is a complete assessment submission, so the topic is an accurate descriptor of the message. In addition, a name like \"test\" or \"test-results\" may cause confusion. Often programmers use \"test\" as a code check or a system check. I did not want the users confusing the topic as a verification. Finally, the topic needed to be concise and easy to type. Unfortunately, there is no good way to abbreviate assessment without it looking silly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many people took *Learning Git*? \n",
    "\n",
    "##### Assumptions\n",
    "\n",
    "   - There is only one assessment per class. Since we know the assessment ID's are unique, if there are multiple exams, it will inflate the number\n",
    "   - The class name is the name of the assessment. We do not have the name of the class, but we do have the name of the assessment.\n",
    "   - A student only take a class once. This is a limitation with our data, we do not know the relationship between and individual student and the exam id's. Therefore, we have to assume that a student only takes a class, and an assessment once.\n",
    "   \n",
    "\n",
    "##### Findings\n",
    " \n",
    "390 students took the class \"learning Git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_exam_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_exam_id\n",
       "0             390"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_git = spark.sql(\"SELECT COUNT(DISTINCT user_exam_id) as unique_exam_id \\\n",
    "                       FROM focused_assessments \\\n",
    "                       WHERE exam_name = 'Learning Git'\")\n",
    "\n",
    "learn_git_pdf = learn_git.select(\"*\").toPandas()\n",
    "learn_git_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the least common course taken? And the most common?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assumptions\n",
    "\n",
    "   - All students enrolled in the class took an assessment\n",
    "   - Students only take the class once\n",
    "   - There is only one assessment in the class\n",
    "   \n",
    "##### Thinking\n",
    "\n",
    "   - SparkSql does not support the ability to use a MAX/MIN function on an aggregate\n",
    "   - I found it difficult to give data fames alias as common errors were the data frame does not exist\n",
    "   - It is possible for multiple classes to have the same number of assessments taken\n",
    "\n",
    "##### Findings\n",
    "\n",
    "   - The most popular class is \"Learning Git\" with 390 assessments completed\n",
    "   - The least popular classes are \"Nulls, Three-valued Logic and Missing Information\", \"Native Web Apps for Android\", \"Learning to visualize Data with D3.js\", and \"Operating Red Hat Enterprise Linux Servers\" all with 1 completed assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exam_name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Learning Git</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Introduction to Python</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Introduction to Java 8</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intermediate Python Programming</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Learning to Program with R</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         exam_name  count\n",
       "0                     Learning Git    390\n",
       "1           Introduction to Python    162\n",
       "2           Introduction to Java 8    158\n",
       "3  Intermediate Python Programming    156\n",
       "4       Learning to Program with R    128"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_pop = spark.sql(\"SELECT exam_name, \\\n",
    "                      COUNT(DISTINCT user_exam_id) AS count \\\n",
    "                      FROM focused_assessments \\\n",
    "                      GROUP BY exam_name \\\n",
    "                      ORDER BY count DESC \\\n",
    "                      limit 5\")\n",
    "\n",
    "most_pop_pdf = most_pop.select(\"*\").toPandas()\n",
    "most_pop_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exam_name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nulls, Three-valued Logic and Missing Information</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Native Web Apps for Android</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Learning to Visualize Data with D3.js</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Operating Red Hat Enterprise Linux Servers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Understanding the Grails 3 Domain Model</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           exam_name  count\n",
       "0  Nulls, Three-valued Logic and Missing Information      1\n",
       "1                        Native Web Apps for Android      1\n",
       "2              Learning to Visualize Data with D3.js      1\n",
       "3         Operating Red Hat Enterprise Linux Servers      1\n",
       "4            Understanding the Grails 3 Domain Model      2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_pop = spark.sql(\"SELECT exam_name, \\\n",
    "                      COUNT(DISTINCT user_exam_id) AS count \\\n",
    "                      FROM focused_assessments \\\n",
    "                      GROUP BY exam_name \\\n",
    "                      ORDER BY count \\\n",
    "                      LIMIT 5\")\n",
    "\n",
    "least_pop_pdf = least_pop.select(\"*\").toPandas()\n",
    "least_pop_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> What are the most popular courses taken for certification? </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assumptions\n",
    "\n",
    "   - All students enrolled in the class took an assessment\n",
    "   - Students only take the class once\n",
    "   - There is only one assessment in the class\n",
    "   - Some educational platforms have a free content and monetize the certification strategy. Knowing how many students that convert to a certification path could increase our value to our customers\n",
    "   \n",
    "##### Thinking\n",
    "\n",
    "   - The most popular courses students are paying for a certification, and potentially higher revenue, may not be the overall most popular courses\n",
    "   - It would be great for us to know and focus on the students who are willing to pay for a certification as a way to increase revenue.\n",
    "   \n",
    "##### Findings\n",
    "\n",
    "   - No students in this data are pursuing a certification. This is a huge gap that should be addressed, especially if the firms monetization strategy is dependent on certifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_name</th>\n",
       "      <th>number_of_students</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [course_name, number_of_students]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cert = spark.sql(\"SELECT exam_name as course_name, \\\n",
    "                    COUNT(DISTINCT user_exam_id) as number_of_students \\\n",
    "                    FROM focused_assessments \\\n",
    "                    WHERE certification='true' \\\n",
    "                    GROUP BY course_name \\\n",
    "                    ORDER BY number_of_students DESC\")\n",
    "\n",
    "cert_pdf = cert.select(\"*\").toPandas()\n",
    "cert_pdf"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
